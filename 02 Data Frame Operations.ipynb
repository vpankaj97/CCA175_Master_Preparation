{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import isnan, when, count, col, translate, lower\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.sql import functions as F\n",
    "spark=SparkSession.builder.appName('BigData').master('yarn').getOrCreate()\n",
    "sc=spark.sparkContext\n",
    "sql=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x7ffe4464c080>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://172.25.192.1:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>BigData</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark.read.csv('/public/retail_db/orders',inferSchema=True).toDF('order_id','order_date','order_total','order_status')\n",
    "order_items = spark.read.csv('/public/retail_db//order_items',header=True,inferSchema=True)\n",
    "customers = spark.read.csv('/public/retail_db//customers',header=True,inferSchema=True)\n",
    "products = spark.read.csv('/public/retail_db//products',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AnalysisException",
     "evalue": "Path does not exist: hdfs://localhost:9000/user/pankaj/retail_db/orders.csv;",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d1b1357900da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0morders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'retail_db//orders.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0morder_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'retail_db//order_items.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcustomers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'retail_db//customers.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mproducts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'retail_db//products.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/BigData/Spark-3.0.0/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup)\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/BigData/Spark-3.0.0/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/BigData/Spark-3.0.0/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/BigData/Spark-3.0.0/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: hdfs://localhost:9000/user/pankaj/retail_db/orders.csv;"
     ]
    }
   ],
   "source": [
    "orders = spark.read.csv('retail_db//orders.csv',header=True,inferSchema=True)\n",
    "order_items = spark.read.csv('retail_db//order_items.csv',header=True,inferSchema=True)\n",
    "customers = spark.read.csv('retail_db//customers.csv',header=True,inferSchema=True)\n",
    "products = spark.read.csv('retail_db//products.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---------------+---------------+---------------+\n|   order_status|   order_status|   order_status|\n+---------------+---------------+---------------+\n|         CLOSED|         CLOSED|         CLOSED|\n|PENDING_PAYMENT|PENDING_PAYMENT|PENDING_PAYMENT|\n|       COMPLETE|       COMPLETE|       COMPLETE|\n|         CLOSED|         CLOSED|         CLOSED|\n|       COMPLETE|       COMPLETE|       COMPLETE|\n|       COMPLETE|       COMPLETE|       COMPLETE|\n|       COMPLETE|       COMPLETE|       COMPLETE|\n|     PROCESSING|     PROCESSING|     PROCESSING|\n|PENDING_PAYMENT|PENDING_PAYMENT|PENDING_PAYMENT|\n|PENDING_PAYMENT|PENDING_PAYMENT|PENDING_PAYMENT|\n| PAYMENT_REVIEW| PAYMENT_REVIEW| PAYMENT_REVIEW|\n|         CLOSED|         CLOSED|         CLOSED|\n|PENDING_PAYMENT|PENDING_PAYMENT|PENDING_PAYMENT|\n|     PROCESSING|     PROCESSING|     PROCESSING|\n|       COMPLETE|       COMPLETE|       COMPLETE|\n|PENDING_PAYMENT|PENDING_PAYMENT|PENDING_PAYMENT|\n|       COMPLETE|       COMPLETE|       COMPLETE|\n|         CLOSED|         CLOSED|         CLOSED|\n|PENDING_PAYMENT|PENDING_PAYMENT|PENDING_PAYMENT|\n|     PROCESSING|     PROCESSING|     PROCESSING|\n+---------------+---------------+---------------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "orders.select(orders.order_status,'order_status',col('order_status')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But while using functions it is applicable to specify the full notation, i.e., dataframe.column_name or using col as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|lower(order_status)|lower(order_status)|lower(order_status)|\n",
      "+-------------------+-------------------+-------------------+\n",
      "|             closed|             closed|             closed|\n",
      "|    pending_payment|    pending_payment|    pending_payment|\n",
      "|           complete|           complete|           complete|\n",
      "|             closed|             closed|             closed|\n",
      "|           complete|           complete|           complete|\n",
      "|           complete|           complete|           complete|\n",
      "|           complete|           complete|           complete|\n",
      "|         processing|         processing|         processing|\n",
      "|    pending_payment|    pending_payment|    pending_payment|\n",
      "|    pending_payment|    pending_payment|    pending_payment|\n",
      "|     payment_review|     payment_review|     payment_review|\n",
      "|             closed|             closed|             closed|\n",
      "|    pending_payment|    pending_payment|    pending_payment|\n",
      "|         processing|         processing|         processing|\n",
      "|           complete|           complete|           complete|\n",
      "|    pending_payment|    pending_payment|    pending_payment|\n",
      "|           complete|           complete|           complete|\n",
      "|             closed|             closed|             closed|\n",
      "|    pending_payment|    pending_payment|    pending_payment|\n",
      "|         processing|         processing|         processing|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.select(lower(orders.order_status),lower(col('order_status')),lower('order_status')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# alias\n",
    "#### alias should be enclosed within select "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note : Please do not give alias the same name as built-in functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[1: string, 2: string, 3: string]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#orders.select(orders.order_status.alias('Status_alias')).show()\n",
    "#Note: orders.select('order_status').alias('Status_alias').show()  will not work\n",
    "orders.select(lower(orders.order_status).alias('1'),lower(col('order_status')).alias('2'),lower('order_status').alias('3'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+---------------+\n",
      "|              1|              2|              3|\n",
      "+---------------+---------------+---------------+\n",
      "|        on_hold|        on_hold|        on_hold|\n",
      "|     processing|     processing|     processing|\n",
      "|       canceled|       canceled|       canceled|\n",
      "|        pending|        pending|        pending|\n",
      "|         closed|         closed|         closed|\n",
      "| payment_review| payment_review| payment_review|\n",
      "|       complete|       complete|       complete|\n",
      "|suspected_fraud|suspected_fraud|suspected_fraud|\n",
      "|pending_payment|pending_payment|pending_payment|\n",
      "+---------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.select(lower(orders.order_status).alias('1'),lower(col('order_status')).alias('2'),lower('order_status').alias('3')).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# withcolumn\n",
    "#### If already existing column name is given, then the new column expression will replace the existing column. (It will not add another column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = orders. \\\n",
    "withColumn('order_id',orders.order_id.cast('bigint')). \\\n",
    "withColumn('order_date',orders.order_date.cast('date')). \\\n",
    "withColumn('order_customer_id',orders.order_customer_id.cast('bigint')). \\\n",
    "withColumn('order_status',col('order_status').cast('string')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders.select('order_id','order_status').withColumn('SomethingNew',orders.order_id+1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given below are the cast notations from pyspark.sql.types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BinaryType: binary\n",
    "BooleanType: boolean\n",
    "ByteType: tinyint\n",
    "DateType: date\n",
    "DecimalType: decimal(10,0)\n",
    "DoubleType: double\n",
    "FloatType: float\n",
    "IntegerType: int\n",
    "LongType: bigint\n",
    "ShortType: smallint\n",
    "StringType: string\n",
    "TimestampType: timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# selectExpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL like expressions can be used for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders.selectExpr('order_id||order_date||\",\"||order_customer_id||\",\"||order_status||\",\" as textdata ').show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# case\n",
    "#### CASE as in SQL can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders.selectExpr('CASE WHEN order_status in (\"COMPLETE\",\"CLOSED\") THEN \"COMPLETELD\" WHEN order_status = \"CANCELED\" THEN \"CANCEL\" ELSE \"NONE\" END Derived_status').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# when\n",
    "#### Dataframe equivalent of case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders.withColumn('Derived',when(orders.order_status.isin('COMPLETE','CLOSED'),'DONEEE').when(orders.order_status=='PENDING_PAYMENT','Gareeb').otherwise('Bhool Ja')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# withColumnRenamed\n",
    "#### columns can be renamed with this API, where the first argument is the existing column name and the second argument is the new name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.withColumnRenamed('order_status','status_of_order').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter as in SQL format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.filter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.filter(\"order_status = 'COMPLETE'\").show()\n",
    "#Note: Single '=' as in SQL format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter as in Dataframe format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.filter(orders.order_status == 'COMPLETE').show()\n",
    "#Note: Equality operator '==' for dataframe filter expression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multiple filters in SQL format filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders.filter(\"order_status = 'COMPLETE' OR order_status = 'CLOSED'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multiple filters in DataFrame format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#orders.filter((orders.order_status == 'COMPLETE').__or__(orders.order_status == 'CLOSED')).show() \n",
    "orders.filter((orders.order_status == 'COMPLETE') | (orders.order_status == 'CLOSED')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Examples of few multiple filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders.filter(\"order_status in ('COMPLETE' ,'CLOSED') AND date_format(order_date,'yyyyMM') = '201308'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#orders.registerTempTable(\"df\")\n",
    "#spark.sql(\"select * from df where order_date like '2013-08%'\").show()\n",
    "\n",
    "orders.filter(orders.order_status.isin('COMPLETE','CLOSED') & orders.order_date.like('2013-08%')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders.filter('order_customer_id >= 1000').filter('order_customer_id != 2321').filter(orders.order_id.between(1000,1999)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### simple inner join with one column mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.join?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'order_item_order_id'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b13e210c749f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0morders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder_id\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0morder_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder_item_order_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/BigData/Spark-3.0.0/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1401\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1402\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'order_item_order_id'"
     ]
    }
   ],
   "source": [
    "orders.join(order_items, orders.order_id==order_items.order_item_order_id).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform join but get results from only the left dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders.join(orders, ['order_id'],how='left_semi').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inner join with multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders.join(order_items, [orders.order_id==order_items.order_item_order_id, orders.order_id==order_items.order_item_order_id]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### left join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customers.join(orders, customers.customer_id==orders.order_customer_id, 'left').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### right join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customers.join(orders, customers.customer_id==orders.order_customer_id, 'right').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### leftanti join - Join to fetch records which exists only in one table\n",
    "In the below case it will fetch the records which are present only in the orders and not in order_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customers.join(orders, orders.order_customer_id==customers.customer_id, 'leftanti').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### crossJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.crossJoin(customers.filter(customers.customer_id == 1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# distinct\n",
    "#### distinct will come at the end after the select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---------------+\n|   order_status|\n+---------------+\n|PENDING_PAYMENT|\n|       COMPLETE|\n|        ON_HOLD|\n| PAYMENT_REVIEW|\n|     PROCESSING|\n|         CLOSED|\n|SUSPECTED_FRAUD|\n|        PENDING|\n|       CANCELED|\n+---------------+\n\n"
    }
   ],
   "source": [
    "orders.select('order_status').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# countDinstinct\n",
    "#### countDistinct will be handy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.select(countDistinct('order_status')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# orderBy/sort\n",
    "#### Note: sort is just an alias to orderBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.orderBy(orders.order_date, orders.order_status.desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Only absolute column name notation or col() works while mentioning in descending order desc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.orderBy('order_date',col('order_status').desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop() will only take just the column names and it will not take any other expressions for the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items.join(products,order_items.order_item_product_id==products.product_id).drop('product_price','product_description','product_image').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#order_items.join(products, order_item_product_id==products.product_id).drop(products.product_price, products.product_description, products.product_image).show()\n",
    "# Specifying full name of the column will not work "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}